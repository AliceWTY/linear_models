---
title: "cross_validation"
author: "Tianyou Wang"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

Although hypothesis tests provide a way to compare nested linear models, in many situations they don’t fit nicely in this paradigm. Indeed, for many modern tools and applications, the emphasis lies on **prediction accuracy rather than on statistical significance**. In these cases, **cross validation** provides a way to **compare the predictive performance of competing methods**.


## CV by "hand"

We’ll start with a simulated example. The code chunk below generates data under a non-linear model. I like to use this setting because “model complexity” is easiest for me to understand when I can see it.

```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - 0.3) ^ 2 + rnorm(100, 0, 0.3)
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point()
```


Spliting this data into training data and testing data (using `anti_join`), and replot it to show the split. Our goal will be to use the *training data (in black)* to build candidate models, and then see how those models predict in the *testing data (in red)*.

```{r}
train_df = sample_n(nonlin_df, 80)
test_df = anti_join(nonlin_df, train_df, by = "id")

ggplot(train_df, aes(x = x, y = y)) + 
  geom_point() + 
  geom_point(data = test_df, color = "red")

```


Fitting three models to the *training data (black)*. I’m going to use `mgcv::gam` for non-linear models (this is my go-to package for “additive models”, and I much prefer it to e.g. polynomial models.) For today, you don’t have to know what this means, how `gam` works, or why I prefer it – just know that **we’re putting smooth lines through data clouds, and we can control how smooth we want the fit to be**.

The three models below have very different levels of complexity and aren’t nested, so testing approaches for nested model don’t apply.

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = mgcv::gam(y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

To understand what these models have done, I’ll plot the two `gam` fits.

```{r}
train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(aes(y = pred), color = "red")


train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + 
  geom_line(aes(y = pred), color = "red")
```

In a case like this, I can also use the handy `modelr::gather_predictions` function, a shortcut to add predictions form several models to a data frame and then “pivoting” so the result is a tidy, “long” dataset that’s easily plottable.

```{r}
train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(aes(y = pred), color = "red") + 
  facet_wrap(~model)
```

A quick visual inspection suggests that the linear model is too simple, the standard `gam` fit is pretty good, and the wiggly `gam` fit is too complex.

Put differently:

* the linear model is too simple and, no matter what training data we use, will never capture the true relationship between variables. It will be consistently wrong due to its simplicity, therefore it's biased. 
* The wiggly fit, on the other hand, is chasing data points and will change a lot from one training dataset to the the next. It will be consistently wrong due to its complexity, and is therefore highly variable.


As a next step in my CV procedure, I’ll compute root mean squared errors (RMSEs) for each model.

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

The RMSEs are suggestive that both nonlinear models work better than the linear model, and that the smooth fit is better than the wiggly fit. However, to get a sense of model stability we really need to iterate this whole process.

The `modelr` has other outcome measures. RMSE is the most common one. Note that the median absolute deviation is also pretty common.


## CV using `modelr`

Luckily, `modelr` has tools to automate elements of the CV process. In particular, `crossv_mc(dataset_for_split, #_of_splits)` preforms the training/testing split multiple times, and stores the datasets using list columns.

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100) 
```

`crossv_mc` tries to be smart about memory, rather than repeating the dataset a bunch of times. It saves the data once and stores the indexes for each training/testing split using a `resample` object. This can be coerced to a dataframe, and can often be treated exactly like a dataframe. However, it’s not compatible with `gam`, so we have to convert each training and testing dataset (and lose that nice memory-saving stuff in the process) using the code below. It’s worth noting that if all the models you want to fit use `lm`, you can skip this.

```{r}
cv_df %>% pull(train) %>% .[[1]] %>% as_tibble

cv_df %>% pull(test) %>% .[[1]] %>% as_tibble

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```


I now have many training and testing datasets. I’d like to fit my candidate models above and assess prediction accuracy as I did for the single training/testing split. To do this, I’ll fit models and obtain RMSEs using `mutate` + `map` & `mutate` +`map2`.

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    linear_mod  = map(train, ~lm(y ~ x, data = .x)),
    smooth_mod  = map(train, ~mgcv::gam(y ~ s(x), data = .x)),
    wiggly_mod  = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)),
    rmse_wiggly = map2_dbl(wiggly_mod, test, ~rmse(model = .x, data = .y)))
```

Use violin plot to show the distribution of RMSE values for each candidate model.

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

Repeating the split is helpful. Now we get a sense of variance in prediction error and can compare prediction error distributions across methods. The smooth fit is a clear winner!

It’s worth note that this isn’t testing a null hypothesis and there aren’t p-values as a result.


## Example: Child Growth

A cross-sectional study of Nepalese children was carried out to understand the relationships between various measures of growth, including weight and arm circumference. 

```{r}
child_growth = read_csv("./data/nepalese_children.csv")

child_growth %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .5)
```

The plots suggests some non-linearity, especially at the low end of the weight distribution. We’ll try three models: **a linear fit**, **a piece-wise linear fit**, and **a smooth fit** using `gam`. For the piece-wise linear fit, we need to add a “change point term” to our dataframe. (Like additive models, for now it’s not critical that you understand everything about a piecewise linear fit – we’ll see a plot of the results soon, and the intuition from that is enough for our purposes.)

```{r}
child_growth =
  child_growth %>% 
  mutate(weight_cp = (weight > 7) * (weight - 7))
```

The code chunk below fits each of the candidate models to the full dataset. The piecewise linear model is nested in the linear model and could be assessed using statistical significance, but the smooth model is not nested in anything else.

```{r}
linear_mod = lm(armc ~ weight, data = child_growth)
pwl_mod    = lm(armc ~ weight + weight_cp, data = child_growth)
smooth_mod = gam(armc ~ s(weight), data = child_growth)
```


Plot the three models to get intuition for goodness of fit.

```{r}
child_growth %>% 
  gather_predictions(linear_mod, pwl_mod, smooth_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(~model)
```

It’s not clear which is best. The linear model is maybe too simple, but the piecewise and non-linear models are pretty similar. We want to check the prediction errors using the same process as before. Since I want to fit a `gam` model, I have to convert the `resample` objects produced by `crossv_mc` to dataframes, but wouldn’t have to do this if I only wanted to compare the linear and piecewise models.

```{r}
cv_df =
  crossv_mc(child_growth, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

Next, fitting models to training data and obtain corresponding RMSEs for the testing data.

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    linear_mod  = map(train, ~lm(armc ~ weight, data = .x)),
    pwl_mod     = map(train, ~lm(armc ~ weight + weight_cp, data = .x)),
    smooth_mod  = map(train, ~gam(armc ~ s(weight), data = as_tibble(.x)))) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, ~rmse(model = .x, data = .y)),
    rmse_pwl    = map2_dbl(pwl_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)))
```

Finally, I’ll plot the prediction error distribution for each candidate model.

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()

```

Based on these results, there’s clearly some improvement in predictive accuracy gained by allowing non-linearity. Among the non-linear models, the *smooth fit* from `gam` might be a bit better than the *piecewise linear model*. Which candidate model is best, though, depends a bit on both the need to balance complexity with goodness of fit and interpretability. In the end, I’d probably go with the piecewise linear model. The non-linearity is clear enough that it should be accounted for, and the differences between the `piecewise` and `gam` fits are small enough that the easy interpretation of the piecewise model “wins”.




