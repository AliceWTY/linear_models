---
title: "linear_models"
author: "Tianyou Wang"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(p8105.datasets)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Import data

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)
```

```{r}
nyc_airbnb %>% 
  ggplot(aes(x=stars, y=price, color=borough)) +
  geom_point()
```

## Model Fitting

The `lm()` function begins with the formula specification – outcome on the left of the `~` and predictors separated by `+` on the right. As we’ll see shortly, interactions between variables can be specified using `*`. You can also specify an **intercept-only** model `(outcome ~ 1)`, a model with **no intercept** `(outcome ~ 0 + ...)`, and a **model using all available predictors** `(outcome ~ .)`.

```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)
```

Let's look at the results.
The output of a `lm` is a very specific list that isn’t a data frame but that can be manipulated using other functions. 

```{r}
fit

summary(fit)
summary(fit)$coef
coef(fit)
```

Using `class()` to check. It is a matrix.
```{r}
summary(fit)$coef %>% class()
```

You can also checked the fitted values and residuals.

```{r eval=FALSE}
fitted.values(fit)
residuals(fit)
```

Let's look at the results better. 

The `broom` package has functions for obtaining a quick summary of the model and for cleaning up the coefficient table. Note: both of these functions produce data frames, which makes it straightforward to include the results in subsequent steps.

```{r}
broom::glance(fit)

broom::tidy(fit) %>% 
knitr::kable(digits = 3)

broom::tidy(fit) %>% 
  select(-std.error, -statistic) %>% 
  mutate(
    term = str_replace(term, "borough", "Brough: ")
    ) %>% 
  knitr::kable(digits = 3)
```

As an aside, `broom::tidy` works with lots of things, including most of the functions for model fitting you’re likely to run into (survival, mixed models, additive models, …).


## Be in controls of factors

R will treat categorical (factor) covariates appropriately and predictably: indicator variables are created for each non-reference category and included in your model, and the factor level is treated as the reference. Usually, alphabetically, the first level is chosen as the reference group.

It’s important to note that changing reference categories won’t change “fit” or statistical sigificance, but can affect ease of interpretation.

Now we want to change the reference `borough` category as the one that have the most rentals. We also want the `room_type` to be treated as a factor variable, not in alphabetical order but treating the reference group as whichever one is most common.

```{r}
nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(
    borough = fct_infreq(borough),
    room_type = fct_infreq(room_type))

nyc_airbnb %>% 
  ggplot(aes(x=stars, y=price, color=borough)) +
  geom_point()

fit = lm(price ~ stars + borough, data = nyc_airbnb)

broom::tidy(fit)
broom::glance(fit)
```

See the graph again, we notice the order of `borough` changed. Manhattan has the most rentals, followed by Brooklyn, Queens, and Bronx. Additionally, we fit the linear model again, Manhattan is the reference group now. The estimators changed. However, we check `broom::glance(fit)`, the model r-sq, AIC, BIC etc. stay the same.


## Diagnositics

Regression diagnostics can identify issues in model fit, especially related to certain failures in model assumptions. Examining residuals and fitted values are therefore an imporant component of any modeling exercise.

The `modelr` package can be used to add residuals and fitted values to a data frame.

Note: `modelr::add_residuals(data, model)`

```{r}
modelr::add_residuals(nyc_airbnb, fit)
modelr::add_predictions(nyc_airbnb, fit)
```


Like many things in the tidyverse, the first argument is a dataframe. That makes it easy to included steps adding residuals or predictions in pipeline of commands to conduct inspections and perform diagnostics.

### Exploratory diagnositics examples
Looking at the distribution of residuals in each of the `borough`'s. We found that some obvious issues, most notably the presence of extremely large outliers in price and a generally skewed residual distribution.
```{r}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = borough, y = resid)) + 
  geom_violin() +
  ylim(-500, 1500)
```

From this following graph, we observed that, in Manhattan, residuals has spread out quite a bit (a lot of outliers which contribute to the skewness) when the stars goes up. Similiar parttern is observed in Brooklyn. Queens has one outlier and we should check on that.

```{r}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = stars, y = resid)) + 
  geom_point() +
  facet_wrap(. ~ borough)

```



## Hypothesis Test

This does t-test by default.

```{r}
fit %>% 
  broom::tidy()
```

What about the significance of `borough` (testing the 3 variables at the same time)? We want F-test or ANOVA.

To do that we need to fit a null model and an alternative model (your current model).

```{r}
fit_null = lm(price ~ stars, data = nyc_airbnb)
fit_alt = lm(price ~ stars + borough, data = nyc_airbnb)

anova(fit_null, fit_alt)
```

We can try to tidy it a bit.

```{r}
anova(fit_null, fit_alt) %>% 
  broom::tidy()
```


## Nest data, fit models

We’ll now turn our attention to fitting models to datasets nested within variables – meaning, essentially, that we’ll use `nest` to create a list column containing datasets and fit separate models to each. This is very different from fitting nested models, even though the terminology is similar.

In the airbnb data, we might think that star ratings and room type affects price differently in each borough. **One way to allow this kind of effect modification is through interaction terms:**

```{r}
nyc_airbnb %>% 
  lm(price ~ stars * borough + room_type * borough, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

```

This works, but the output takes time to think through – the expected change in price comparing an entire apartment to a private room in Queens, for example, involves the main effect of room type and the Queens / private room interaction.


Alternatively, we can nest within boroughs and fit borough-specific models associating price with rating and room type. This is more exploratory but easier to understand.

```{r}
nest_lm_res =
  nyc_airbnb %>% 
  nest(data = -borough) %>% 
  mutate(
    models = map(data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)
```

Note:

* `nest(data = -borough)` nest everything expect `borough`
* mapping, `map(data_entry, ~function)`


Showing the results in wide format.

```{r}
nest_lm_res %>% 
  select(borough, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  knitr::kable(digits = 3)
```

Fitting models to nested datasets is a way of performing **stratified analyses**. These have a tradeoff: stratified models make it easy to interpret covariate effects in each stratum, but *don’t provide a mechanism for assessing the significance of differences across strata*.


An even more extreme example is the assessment of neighborhood effects in Manhattan. The code chunk below fits neighborhood-specific models:

```{r}
manhattan_nest_lm_res =
  nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  nest(data = -neighborhood) %>% 
  mutate(
    models = map(data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)
```

Showing neighborhood-specific estimates for the coefficients related to room type. We found that there is, generally speaking, a reduction in room price for a private room or a shared room compared to an entire apartment, but this varies quite a bit across neighborhoods.

```{r}
manhattan_nest_lm_res %>% 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```

Note:

* `str_detect(term, "room_type")` detect data that start with "room_type" in the `term` variable
* `theme(axis.text.x = element_text(angle = 80, hjust = 1))` rotating and spacing exis labels in `ggplot2`


### mixed model

With this many factor levels, it really isn’t a good idea to fit models with main effects or interactions for each. Instead, you’d be best-off using a **mixed model**, with **random intercepts and slopes for each neighborhood**. Although it’s well beyond the scope of this class, code to fit a mixed model with neighborhood-level random intercepts and random slopes for room type is below. And, of course, we can tidy the results using a mixed-model spinoff of the `broom` package.

Mixed-effects models are called “mixed” because they simultaneously model fixed and random effects.

```{r eval=FALSE}
manhattan_airbnb =
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = .)%>% 
  broom.mixed::tidy()
```



## Binary Outcomes

Linear models are appropriate for outcomes that follow a continuous distribution, but **binary outcomes** are common. In these cases, **logistic regression** is a useful analytic framework.

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository. We’ll use data on unresolved murders in Baltimore, MD to illustrate logistic regression in R. The code below imports, cleans, and generally wrangles the data for analysis.

```{r}
baltimore_df = 
  read_csv("data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

Using these data, we can fit a logistic regression for the binary “resolved” outcome and victim demographics as predictors. This uses the `glm` function with the `family=assumed_distribution` specified to account for the non-Gaussian outcome distribution.

```{r}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
```


Many of the same tools we used to work with `lm` fits can be used for `glm` fits. The table below summaries the coefficients from the model fit; because logistic model estimates are log odds ratios, we **include a step to compute odds ratios** as well.

```{r}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```

Homicides in which the victim is Black are substantially less likely to be resolved that those in which the victim is white; for other races the effects are not significant, possible due to small sample sizes. Homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female. The effect of age is statistically significant, but careful data inspections should be conducted before interpreting too deeply.


We can also compute fitted values; similarly to the estimates in the model summary, these are expressed as log odds and can be transformed to produce probabilities for each subject. Note: the `pred` is the odds, we want to transfer it into probabilities.

```{r}
baltimore_df %>% 
  modelr::add_predictions(fit_logistic) %>% 
  mutate(fitted_prob = boot::inv.logit(pred))
```


